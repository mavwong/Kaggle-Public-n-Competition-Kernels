{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3317c9c2e6fe7f0cb7932a067ebc8bb2367f99b9"
   },
   "source": [
    "## Overview\n",
    "\n",
    "  - Importing Dependencies\n",
    "  - Data / Data Generator\n",
    "  - Data Exploration / Data Visualization\n",
    "  - Data Augmentation\n",
    "  - Callbacks\n",
    "  - Model Architecture (Model -> Compile -> Fit)\n",
    "  - Evaluation\n",
    "  - Showing the Result\n",
    "  - Saving and Loading the Mode\n",
    "  - Testing the Model\n",
    "  - Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "79b47a17c95ee60eb20ea2c1f86e43ce731ac557"
   },
   "source": [
    "## Note:\n",
    "\n",
    "In a starting a new project, normally, I would start with a simple neural network and a data. The overview would look something like:\n",
    "  - Importing the Dependencies\n",
    "  - Data\n",
    "  - Model Architecture (Model -> Compile -> Fit)\n",
    "  - Evaluation\n",
    "  - Showing the Result\n",
    "  \n",
    "In creating a more accurate model, you need to evaluate the training accuracy and the validation accuracy of the model, thus training losses and validation losses. As of my knowledge today, there two concept problems you need to identify in your evaluation for creating a more accurate complex network and dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "411240eb2f8c7502a59601444130f33a887f0e8a"
   },
   "source": [
    "### **Importing Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "766415a7cf84fd1996887ca0cc0e1582828c3878"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "8151328c5febc37c5f668e9cc5b0b27c957e8342",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting livelossplot\r\n",
      "  Downloading https://files.pythonhosted.org/packages/07/9d/54f8a93d65eece0bcd475b191c4c9a3bff9dbf993db8d5e2d02b76c2d2c3/livelossplot-0.3.3-py3-none-any.whl\r\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.6/site-packages (from livelossplot) (5.5.0)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from livelossplot) (3.0.3)\r\n",
      "Requirement already satisfied: traitlets>=4.2.1 in /opt/conda/lib/python3.6/site-packages (from notebook->livelossplot) (4.3.2)\r\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.6/site-packages (from notebook->livelossplot) (0.2.0)\r\n",
      "Requirement already satisfied: tornado>=4 in /opt/conda/lib/python3.6/site-packages (from notebook->livelossplot) (5.0.2)\r\n",
      "Requirement already satisfied: Send2Trash in /opt/conda/lib/python3.6/site-packages (from notebook->livelossplot) (1.5.0)\r\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.6/site-packages (from notebook->livelossplot) (17.0.0)\r\n",
      "Requirement already satisfied: terminado>=0.8.1 in /opt/conda/lib/python3.6/site-packages (from notebook->livelossplot) (0.8.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.6/site-packages (from notebook->livelossplot) (2.10)\r\n",
      "Requirement already satisfied: jupyter-core>=4.4.0 in /opt/conda/lib/python3.6/site-packages (from notebook->livelossplot) (4.4.0)\r\n",
      "Requirement already satisfied: nbformat in /opt/conda/lib/python3.6/site-packages (from notebook->livelossplot) (4.4.0)\r\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.6/site-packages (from notebook->livelossplot) (5.3.1)\r\n",
      "Requirement already satisfied: jupyter-client>=5.2.0 in /opt/conda/lib/python3.6/site-packages (from notebook->livelossplot) (5.2.3)\r\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.6/site-packages (from notebook->livelossplot) (4.8.2)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->livelossplot) (1.0.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->livelossplot) (2.6.0)\r\n",
      "Requirement already satisfied: numpy>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib->livelossplot) (1.16.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->livelossplot) (0.10.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->livelossplot) (2.2.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from traitlets>=4.2.1->notebook->livelossplot) (1.12.0)\r\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.6/site-packages (from traitlets>=4.2.1->notebook->livelossplot) (4.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from jinja2->notebook->livelossplot) (1.0)\r\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.6/site-packages (from nbformat->notebook->livelossplot) (2.6.0)\r\n",
      "Requirement already satisfied: mistune>=0.7.4 in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook->livelossplot) (0.8.3)\r\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook->livelossplot) (2.2.0)\r\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook->livelossplot) (0.2.3)\r\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook->livelossplot) (2.1.3)\r\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook->livelossplot) (1.4.2)\r\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook->livelossplot) (0.3.1)\r\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from ipykernel->notebook->livelossplot) (6.4.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->livelossplot) (39.1.0)\r\n",
      "Requirement already satisfied: html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre in /opt/conda/lib/python3.6/site-packages (from bleach->nbconvert->notebook->livelossplot) (1.0.1)\r\n",
      "Requirement already satisfied: simplegeneric>0.8 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot) (0.8.1)\r\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot) (0.1.0)\r\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot) (0.7.4)\r\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot) (4.5.0)\r\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot) (0.12.0)\r\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.15 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot) (1.0.15)\r\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.6/site-packages (from html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre->bleach->nbconvert->notebook->livelossplot) (0.5.1)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0->ipykernel->notebook->livelossplot) (0.5.2)\r\n",
      "Requirement already satisfied: parso>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from jedi>=0.10->ipython>=4.0.0->ipykernel->notebook->livelossplot) (0.2.0)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from prompt-toolkit<2.0.0,>=1.0.15->ipython>=4.0.0->ipykernel->notebook->livelossplot) (0.1.7)\r\n",
      "Installing collected packages: livelossplot\r\n",
      "Successfully installed livelossplot-0.3.3\r\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "!pip install livelossplot\n",
    "from livelossplot import PlotLossesKeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d66bcf99a9d6c9579a83b0e9419778b63793ea9c"
   },
   "source": [
    "### **Output Files / Parameters of the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "d187f91e7b191a29295afafdbcb53f76b8e83514"
   },
   "outputs": [],
   "source": [
    "TRAINING_LOGS_FILE = 'training_logs.csv'\n",
    "MODEL_SUMMARY_FILE = 'model.summary.txt'\n",
    "MODEL_FILE = 'histopathologic_cancer_detector.h5'\n",
    "\n",
    "TRAINING_PLOT_FILE = 'training.png'\n",
    "VALIDATION_PLOT_FILE = 'validation.png'\n",
    "ROC_PLOT_FILE = 'roc.png'\n",
    "\n",
    "INPUT_DIRECTORY = '../input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "cccf448a1d1a41a229b6e1b3590eb501c7340e00"
   },
   "outputs": [],
   "source": [
    "#Parameters of the Inputs\n",
    "IMG_HEIGHT = 150\n",
    "IMG_WIDTH = 150\n",
    "IMG_CHANNEL = 3\n",
    "IMG_SIZE = (IMG_HEIGHT, IMG_WIDTH)\n",
    "BATCH_SIZE = 216\n",
    "\n",
    "SAMPLE_COUNT = 85000\n",
    "TRAINING_RATIO = 0.9\n",
    "VERBOSITY = 1\n",
    "TESTING_BATCH_SIZE = 5000\n",
    "NUM_EPOCH = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4bbacef2007d051751654f6949871d2e13fc6c7f"
   },
   "source": [
    "### **Data & Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "057fcb4fae874f57e1b8bafad6268f727b369b90"
   },
   "outputs": [],
   "source": [
    "#Data (Preparing the Training Data)\n",
    "filenames = os.listdir(\"../input/train/train\")\n",
    "categories = []\n",
    "\n",
    "for filename in filenames:\n",
    "    category = filename.split('.')[0]\n",
    "    if category == 'dog':\n",
    "        categories.append(1)\n",
    "    else:\n",
    "        categories.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "9921aa92c812518c1c21001d56b1388bc6e2ed70"
   },
   "outputs": [],
   "source": [
    "categories = [str(i) for i in categories]\n",
    "\n",
    "df = pd.DataFrame({'filename': filenames,\n",
    "                  'category': categories})\n",
    "\n",
    "df['category'] = df['category'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "e07a1360c7a45c035fe5fbce6e990babdab76200"
   },
   "outputs": [],
   "source": [
    "#Splitting the data\n",
    "train_dataframe, validate_dataframe = train_test_split(df, test_size=0.20, \n",
    "                                                         random_state=42)\n",
    "\n",
    "train_dataframe = train_dataframe.reset_index(drop=True)\n",
    "validate_dataframe = validate_dataframe.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "55acede497b30b1ac9564e3adb0dea6f67e7bd47",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Data Preparation / Generator(Scaling ONLY)\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_dataframe(train_dataframe,\n",
    "                                                       '../input/train/train',\n",
    "                                                       x_col='filename',\n",
    "                                                       y_col='category',\n",
    "                                                       target_size=IMG_SIZE,\n",
    "                                                       class_mode='binary',\n",
    "                                                       batch_size=BATCH_SIZE)\n",
    "\n",
    "validate_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validate_generator = validate_datagen.flow_from_dataframe(validate_dataframe,\n",
    "                                                                 '../input/train/train/',\n",
    "                                                                 x_col='filename',\n",
    "                                                                 y_col='category',\n",
    "                                                                 target_size=IMG_SIZE,\n",
    "                                                                 class_mode='binary',\n",
    "                                                                 batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1d6d1bea9a1dbd3960b1252a3eae8ad97b7f3765"
   },
   "source": [
    "*In this project, I will try to iterate the model architecture, the data, the callbacks, regularization and optimizer. The project will start from a simple neural network with Keras framework, Afterthat, we work towards achieving higher accuracy through iteration process. The project was inspired by Andrew Ng's iterative machine learning cycle of Idea-Code-Experiment.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ce9bdf4ddae5197fb38e5b05510dcabc1e4249a"
   },
   "source": [
    "## ***1st Iteration*** - Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18d70e7cf8dd4e699e95fc9a8e29ce04b4e1f5b1"
   },
   "source": [
    "### **Neural Network Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "aed545f0f63b461d8a7bcf89f305da12e6afb1aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#Model Architecture\n",
    "model_1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Convolution2D(128, kernel_size=(3, 3), input_shape=(IMG_WIDTH, IMG_HEIGHT, IMG_CHANNEL), activation='relu', padding='SAME'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='SAME'),\n",
    "\n",
    "    tf.keras.layers.Convolution2D(64, kernel_size=(3, 3), activation='relu', padding='SAME'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='SAME'),\n",
    "\n",
    "    tf.keras.layers.Convolution2D(64, kernel_size=(3, 3), activation='relu', padding='SAME'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='SAME'),\n",
    "    \n",
    "    tf.keras.layers.Convolution2D(32, kernel_size=(3, 3), activation='relu', padding='SAME'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='SAME'),\n",
    "\n",
    "    tf.keras.layers.Convolution2D(32, kernel_size=(3, 3), activation='relu', padding='SAME'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='SAME'),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "a066794805486841857f5812c9c6ce4eff644c4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learning_rate = 0.0001\\nnum_epochs = 25\\n\\nfrom keras.optimizers import SGD\\n\\noption_1 = SGD(lr=learning_rate)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "'''learning_rate = 0.0001\n",
    "num_epochs = 25\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "option_1 = SGD(lr=learning_rate)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "a7ea40d376fca3bae2c80c40a993d0713e3c9789"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "model_1.compile(loss='binary_crossentropy',\n",
    "              optimizer= 'sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = [PlotLossesKeras()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "f5e2615967a6e1859c99c0fc83b8a3c2f86db582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "24/24 [==============================] - 26s 1s/step - loss: 0.6910 - acc: 0.5640\n",
      "93/93 [==============================] - 123s 1s/step - loss: 0.6916 - acc: 0.5422 - val_loss: 0.6910 - val_acc: 0.5640\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 19s 801ms/step - loss: 0.6891 - acc: 0.5700\n",
      "93/93 [==============================] - 83s 891ms/step - loss: 0.6897 - acc: 0.5765 - val_loss: 0.6891 - val_acc: 0.5700\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - 20s 834ms/step - loss: 0.6870 - acc: 0.5878\n",
      "93/93 [==============================] - 85s 914ms/step - loss: 0.6877 - acc: 0.5824 - val_loss: 0.6870 - val_acc: 0.5878\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - 19s 810ms/step - loss: 0.6846 - acc: 0.5740\n",
      "93/93 [==============================] - 85s 911ms/step - loss: 0.6855 - acc: 0.5891 - val_loss: 0.6846 - val_acc: 0.5740\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 19s 806ms/step - loss: 0.6817 - acc: 0.5934\n",
      "93/93 [==============================] - 84s 906ms/step - loss: 0.6829 - acc: 0.5890 - val_loss: 0.6817 - val_acc: 0.5934\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 19s 803ms/step - loss: 0.6776 - acc: 0.6048\n",
      "93/93 [==============================] - 84s 900ms/step - loss: 0.6795 - acc: 0.5900 - val_loss: 0.6776 - val_acc: 0.6048\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 20s 840ms/step - loss: 0.6742 - acc: 0.5828\n",
      "93/93 [==============================] - 86s 920ms/step - loss: 0.6750 - acc: 0.5942 - val_loss: 0.6742 - val_acc: 0.5828\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 20s 824ms/step - loss: 0.6675 - acc: 0.6012\n",
      "93/93 [==============================] - 85s 917ms/step - loss: 0.6706 - acc: 0.5920 - val_loss: 0.6675 - val_acc: 0.6012\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 20s 813ms/step - loss: 0.6674 - acc: 0.5878\n",
      "93/93 [==============================] - 85s 914ms/step - loss: 0.6673 - acc: 0.5919 - val_loss: 0.6674 - val_acc: 0.5878\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 19s 804ms/step - loss: 0.6561 - acc: 0.6104\n",
      "93/93 [==============================] - 84s 900ms/step - loss: 0.6637 - acc: 0.5967 - val_loss: 0.6561 - val_acc: 0.6104\n"
     ]
    }
   ],
   "source": [
    "first_history = model_1.fit_generator(train_generator,\n",
    "                              steps_per_epoch=len(train_generator), \n",
    "                              validation_data=validate_generator, \n",
    "                              validation_steps=len(validate_generator), \n",
    "                              epochs=NUM_EPOCH, verbose=VERBOSITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8464e00a9750ab71eb80c560021f653ae6090c33"
   },
   "source": [
    "## ***2nd Iteration*** - Training Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "1ecca125678d10eada90004a19df19cd32704f61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Data Generator / Augmentation\n",
    "train_datagen_augmented = ImageDataGenerator(rescale=1./255,\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=0.2,\n",
    "                                    rotation_range=30,\n",
    "                                    horizontal_flip=True,\n",
    "                                    vertical_flip=True,\n",
    "                                    width_shift_range=0.2,\n",
    "                                    height_shift_range=0.2,\n",
    "                                    data_format='channels_last')\n",
    "\n",
    "train_generator_augmented = train_datagen_augmented.flow_from_dataframe(train_dataframe,\n",
    "                                                                           '../input/train/train',\n",
    "                                                                           x_col='filename',\n",
    "                                                                           y_col='category',\n",
    "                                                                           target_size=IMG_SIZE,\n",
    "                                                                           class_mode='binary',\n",
    "                                                                           batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "d9f1be84c315227023e4b83ad99aa64f8ad3e0b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#Callbacks\\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger, ModelCheckpoint\\n\\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, \\n                  patience=5, verbose=0, mode='auto', \\n                  min_delta=0.0001, cooldown=0, min_lr=0)\\n\\ncsv_logger = CSVLogger(TRAINING_LOGS_FILE, separator=',', append=False)\\n\\nmodel_checkpoint = ModelCheckpoint(MODEL_FILE, monitor='val_loss', \\n                                verbose=0, save_best_only=True, \\n                                save_weights_only=False, mode='auto', \\n                                period=1)\\n\\ncallbacks = [reduce_lr, csv_logger, model_checkpoint]\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#Callbacks\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger, ModelCheckpoint\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, \n",
    "                  patience=5, verbose=0, mode='auto', \n",
    "                  min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "csv_logger = CSVLogger(TRAINING_LOGS_FILE, separator=',', append=False)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(MODEL_FILE, monitor='val_loss', \n",
    "                                verbose=0, save_best_only=True, \n",
    "                                save_weights_only=False, mode='auto', \n",
    "                                period=1)\n",
    "\n",
    "callbacks = [reduce_lr, csv_logger, model_checkpoint]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "e324f19da9c1e32d4cf0337fbfafc3e06e6e1243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 [==============================] - 23s 969ms/step - loss: 0.6669 - acc: 0.5920\n",
      "93/93 [==============================] - 173s 2s/step - loss: 0.6761 - acc: 0.5703 - val_loss: 0.6669 - val_acc: 0.5920\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 24s 986ms/step - loss: 0.6527 - acc: 0.6114\n",
      "93/93 [==============================] - 158s 2s/step - loss: 0.6721 - acc: 0.5811 - val_loss: 0.6527 - val_acc: 0.6114\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - 23s 965ms/step - loss: 0.6711 - acc: 0.5662\n",
      "93/93 [==============================] - 158s 2s/step - loss: 0.6690 - acc: 0.5861 - val_loss: 0.6711 - val_acc: 0.5662\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - 23s 962ms/step - loss: 0.6500 - acc: 0.6112\n",
      "93/93 [==============================] - 158s 2s/step - loss: 0.6669 - acc: 0.5913 - val_loss: 0.6500 - val_acc: 0.6112\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 23s 965ms/step - loss: 0.6602 - acc: 0.5858\n",
      "93/93 [==============================] - 158s 2s/step - loss: 0.6675 - acc: 0.5893 - val_loss: 0.6602 - val_acc: 0.5858\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 24s 988ms/step - loss: 0.6621 - acc: 0.5860\n",
      "93/93 [==============================] - 159s 2s/step - loss: 0.6667 - acc: 0.5885 - val_loss: 0.6621 - val_acc: 0.5860\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 23s 965ms/step - loss: 0.6519 - acc: 0.6104\n",
      "93/93 [==============================] - 158s 2s/step - loss: 0.6654 - acc: 0.5895 - val_loss: 0.6519 - val_acc: 0.6104\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 24s 994ms/step - loss: 0.6447 - acc: 0.6096\n",
      "93/93 [==============================] - 158s 2s/step - loss: 0.6622 - acc: 0.5932 - val_loss: 0.6447 - val_acc: 0.6096\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 23s 960ms/step - loss: 0.6467 - acc: 0.6146\n",
      "93/93 [==============================] - 157s 2s/step - loss: 0.6636 - acc: 0.5925 - val_loss: 0.6467 - val_acc: 0.6146\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 25s 1s/step - loss: 0.6441 - acc: 0.6172\n",
      "93/93 [==============================] - 160s 2s/step - loss: 0.6626 - acc: 0.5929 - val_loss: 0.6441 - val_acc: 0.6172\n"
     ]
    }
   ],
   "source": [
    "second_history = model_1.fit_generator(train_generator_augmented,\n",
    "                              steps_per_epoch=len(train_generator_augmented), \n",
    "                              validation_data=validate_generator, \n",
    "                              validation_steps=len(validate_generator), \n",
    "                              epochs=NUM_EPOCH, verbose=VERBOSITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b04ca4bc82d392c857fc081685d6cb0f98d951c"
   },
   "source": [
    "## ***3rd Iteration*** - Regularization and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "df206eb2f6cb1e5e9e28f228149e4bbc4389d2ad"
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "'''model.add(Dense(64, input_dim=64,\n",
    "                kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01)))'''\n",
    "\n",
    "model_3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Convolution2D(128, kernel_size=(3, 3), input_shape=(IMG_WIDTH, IMG_HEIGHT, IMG_CHANNEL), activation='relu', padding='SAME'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='SAME'),\n",
    "    \n",
    "    tf.keras.layers.Convolution2D(64, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01), padding='SAME'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='SAME'),\n",
    "\n",
    "    tf.keras.layers.Convolution2D(64, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01), padding='SAME'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='SAME'),\n",
    "\n",
    "    tf.keras.layers.Convolution2D(32, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01), padding='SAME'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='SAME'),\n",
    "\n",
    "    tf.keras.layers.Convolution2D(32, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01), padding='SAME'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='SAME'),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "223f64d43631746d8e6d70c4a953930e2c00c19e"
   },
   "outputs": [],
   "source": [
    "model_3.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "d8bed9260d4f058d01fa4963deb7639a2511ea15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 [==============================] - 23s 975ms/step - loss: 0.7080 - acc: 0.5010\n",
      "93/93 [==============================] - 171s 2s/step - loss: 1.1941 - acc: 0.5163 - val_loss: 0.7080 - val_acc: 0.5010\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 23s 964ms/step - loss: 0.6933 - acc: 0.4990\n",
      "93/93 [==============================] - 159s 2s/step - loss: 0.6962 - acc: 0.4954 - val_loss: 0.6933 - val_acc: 0.4990\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - 23s 961ms/step - loss: 0.6932 - acc: 0.4990\n",
      "93/93 [==============================] - 157s 2s/step - loss: 0.6932 - acc: 0.4938 - val_loss: 0.6932 - val_acc: 0.4990\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - 23s 958ms/step - loss: 0.6932 - acc: 0.4990\n",
      "93/93 [==============================] - 158s 2s/step - loss: 0.6932 - acc: 0.4951 - val_loss: 0.6932 - val_acc: 0.4990\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 23s 951ms/step - loss: 0.6932 - acc: 0.4990\n",
      "93/93 [==============================] - 156s 2s/step - loss: 0.6932 - acc: 0.4988 - val_loss: 0.6932 - val_acc: 0.4990\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 23s 963ms/step - loss: 0.6932 - acc: 0.4990\n",
      "93/93 [==============================] - 158s 2s/step - loss: 0.6932 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.4990\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 23s 963ms/step - loss: 0.6932 - acc: 0.4990\n",
      "93/93 [==============================] - 156s 2s/step - loss: 0.6932 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.4990\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 23s 967ms/step - loss: 0.6932 - acc: 0.4990\n",
      "93/93 [==============================] - 158s 2s/step - loss: 0.6932 - acc: 0.4924 - val_loss: 0.6932 - val_acc: 0.4990\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 23s 965ms/step - loss: 0.6932 - acc: 0.4990\n",
      "93/93 [==============================] - 157s 2s/step - loss: 0.6932 - acc: 0.4960 - val_loss: 0.6932 - val_acc: 0.4990\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 23s 965ms/step - loss: 0.6932 - acc: 0.4990\n",
      "93/93 [==============================] - 159s 2s/step - loss: 0.6932 - acc: 0.4961 - val_loss: 0.6932 - val_acc: 0.4990\n"
     ]
    }
   ],
   "source": [
    "third_history = model_3.fit_generator(train_generator_augmented,\n",
    "                              steps_per_epoch=len(train_generator_augmented), \n",
    "                              validation_data=validate_generator, \n",
    "                              validation_steps=len(validate_generator), \n",
    "                              epochs=NUM_EPOCH, verbose=VERBOSITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec31efeb7baa80e429bb07fd8f0ddbbd8d74232a"
   },
   "source": [
    "## ***4th Iteration*** - Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "a6eaa16e251049805eaadbcab3f1e943cd292409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "\n",
    "initial_model = VGG19(include_top=False, weights='imagenet', \n",
    "                      input_tensor=None,  input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "53016bf6c2c7d51a8dd7bd7ace48f1c516c3af02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "'''model_4 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.add(initial_model),\n",
    "\n",
    "    tf.keras.layers.add(Flatten()),\n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    #tf.keras.layers.Dense(512, activation='relu'),\n",
    "    #tf.keras.layers.Dropout(0.5)\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])'''\n",
    "\n",
    "model_4 = Sequential()\n",
    "model_4.add(initial_model)\n",
    "\n",
    "model_4.add(Flatten())\n",
    "model_4.add(Dense(1024,activation='relu'))\n",
    "model_4.add(Dropout(0.5))\n",
    "model_4.add(Dense(1,activation='sigmoid'))\n",
    "#model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "2bc3de66e9b28f0fe5908304e55c29df229837fa"
   },
   "outputs": [],
   "source": [
    "model_4.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "30327de5a8a15032c1bbab49136c1b55c71aeea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "93/93 [==============================] - 192s 2s/step - loss: 7.9754 - acc: 0.5005 - val_loss: 8.0429 - val_acc: 0.5010\n",
      "Epoch 2/10\n",
      "93/93 [==============================] - 177s 2s/step - loss: 8.0614 - acc: 0.4999 - val_loss: 8.0429 - val_acc: 0.5010\n",
      "Epoch 3/10\n",
      "93/93 [==============================] - 178s 2s/step - loss: 8.0680 - acc: 0.4994 - val_loss: 8.0429 - val_acc: 0.5010\n",
      "Epoch 4/10\n",
      "93/93 [==============================] - 178s 2s/step - loss: 8.0620 - acc: 0.4998 - val_loss: 8.0429 - val_acc: 0.5010\n",
      "Epoch 5/10\n",
      "93/93 [==============================] - 179s 2s/step - loss: 8.0647 - acc: 0.4996 - val_loss: 8.0429 - val_acc: 0.5010\n",
      "Epoch 6/10\n",
      "93/93 [==============================] - 178s 2s/step - loss: 8.0663 - acc: 0.4995 - val_loss: 8.0429 - val_acc: 0.5010\n",
      "Epoch 7/10\n",
      "93/93 [==============================] - 180s 2s/step - loss: 8.0669 - acc: 0.4995 - val_loss: 8.0429 - val_acc: 0.5010\n",
      "Epoch 8/10\n",
      "93/93 [==============================] - 178s 2s/step - loss: 8.0631 - acc: 0.4998 - val_loss: 8.0429 - val_acc: 0.5010\n",
      "Epoch 9/10\n",
      "93/93 [==============================] - 178s 2s/step - loss: 8.0636 - acc: 0.4997 - val_loss: 8.0429 - val_acc: 0.5010\n",
      "Epoch 10/10\n",
      "93/93 [==============================] - 177s 2s/step - loss: 8.0680 - acc: 0.4994 - val_loss: 8.0429 - val_acc: 0.5010\n"
     ]
    }
   ],
   "source": [
    "fourth_history = model_4.fit_generator(train_generator_augmented,\n",
    "                              steps_per_epoch=len(train_generator_augmented), \n",
    "                              validation_data=validate_generator, \n",
    "                              validation_steps=len(validate_generator), \n",
    "                              epochs=NUM_EPOCH, verbose=VERBOSITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***5th Iteration*** - Freezing Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model_5 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.add(initial_model)\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5)\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])'''\n",
    "\n",
    "model_5 = Sequential()\n",
    "model_5.add(initial_model)\n",
    "\n",
    "model_5.add(Flatten())\n",
    "model_5.add(Dense(1024,activation='relu'))\n",
    "model_5.add(Dropout(0.5))\n",
    "model_5.add(Dense(1,activation='sigmoid'))\n",
    "#model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for layer in initial_model.layers:\\n    print(layer,layer.trainable)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for layer in initial_model.layers[:-15]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "'''for layer in initial_model.layers:\n",
    "    print(layer,layer.trainable)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "93/93 [==============================] - 179s 2s/step - loss: 8.0606 - acc: 0.4990 - val_loss: 8.0107 - val_acc: 0.5030\n",
      "Epoch 2/10\n",
      "93/93 [==============================] - 169s 2s/step - loss: 8.0886 - acc: 0.4973 - val_loss: 8.0075 - val_acc: 0.5032\n",
      "Epoch 3/10\n",
      "93/93 [==============================] - 168s 2s/step - loss: 7.9757 - acc: 0.5043 - val_loss: 8.1396 - val_acc: 0.4950\n",
      "Epoch 4/10\n",
      "93/93 [==============================] - 169s 2s/step - loss: 8.0374 - acc: 0.5005 - val_loss: 7.9914 - val_acc: 0.5042\n",
      "Epoch 5/10\n",
      "93/93 [==============================] - 168s 2s/step - loss: 8.0602 - acc: 0.4990 - val_loss: 8.0784 - val_acc: 0.4988\n",
      "Epoch 6/10\n",
      "93/93 [==============================] - 170s 2s/step - loss: 8.0794 - acc: 0.4978 - val_loss: 8.0204 - val_acc: 0.5024\n",
      "Epoch 7/10\n",
      "93/93 [==============================] - 169s 2s/step - loss: 7.9830 - acc: 0.5039 - val_loss: 8.0204 - val_acc: 0.5024\n",
      "Epoch 8/10\n",
      "93/93 [==============================] - 170s 2s/step - loss: 7.9810 - acc: 0.5040 - val_loss: 8.0816 - val_acc: 0.4986\n",
      "Epoch 9/10\n",
      "93/93 [==============================] - 168s 2s/step - loss: 8.0778 - acc: 0.4979 - val_loss: 8.0494 - val_acc: 0.5006\n",
      "Epoch 10/10\n",
      "93/93 [==============================] - 169s 2s/step - loss: 8.0472 - acc: 0.4999 - val_loss: 8.0042 - val_acc: 0.5034\n"
     ]
    }
   ],
   "source": [
    "fifth_history = model_5.fit_generator(train_generator_augmented,\n",
    "                              steps_per_epoch=len(train_generator_augmented), \n",
    "                              validation_data=validate_generator, \n",
    "                              validation_steps=len(validate_generator), \n",
    "                              epochs=NUM_EPOCH, verbose=VERBOSITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***6th Iteration***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Convolution2D(32, kernel_size=(3, 3), input_shape=(IMG_WIDTH, IMG_HEIGHT, IMG_CHANNEL), activation='relu', padding='SAME'),\n",
    "    tf.keras.layers.Convolution2D(32, kernel_size=(3, 3), activation='relu', padding='SAME'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='SAME'),\n",
    "\n",
    "    tf.keras.layers.Convolution2D(64, kernel_size=(3, 3), activation='relu', padding='SAME'),\n",
    "    tf.keras.layers.Convolution2D(64, kernel_size=(3, 3), activation='relu', padding='SAME'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='SAME'),\n",
    "\n",
    "    tf.keras.layers.Convolution2D(128, kernel_size=(3, 3), activation='relu', padding='SAME'),\n",
    "    tf.keras.layers.Convolution2D(128, kernel_size=(3, 3), activation='relu', padding='SAME'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='SAME'),\n",
    "    \n",
    "    tf.keras.layers.Convolution2D(256, kernel_size=(3, 3), activation='relu', padding='SAME'),\n",
    "    tf.keras.layers.Convolution2D(256, kernel_size=(3, 3), activation='relu', padding='SAME'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='SAME'),\n",
    "\n",
    "    tf.keras.layers.Convolution2D(512, kernel_size=(3, 3), activation='relu', padding='SAME'),\n",
    "    tf.keras.layers.Convolution2D(512, kernel_size=(3, 3), activation='relu', padding='SAME'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='SAME'),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 [==============================] - 25s 1s/step - loss: 0.6931 - acc: 0.5010\n",
      "93/93 [==============================] - 174s 2s/step - loss: 0.6934 - acc: 0.4967 - val_loss: 0.6931 - val_acc: 0.5010\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 25s 1s/step - loss: 0.6931 - acc: 0.5010\n",
      "93/93 [==============================] - 159s 2s/step - loss: 0.6933 - acc: 0.4954 - val_loss: 0.6931 - val_acc: 0.5010\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - 24s 1s/step - loss: 0.6932 - acc: 0.4990\n",
      "93/93 [==============================] - 159s 2s/step - loss: 0.6932 - acc: 0.5023 - val_loss: 0.6932 - val_acc: 0.4990\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - 24s 1s/step - loss: 0.6932 - acc: 0.5010\n",
      "93/93 [==============================] - 158s 2s/step - loss: 0.6932 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.5010\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 25s 1s/step - loss: 0.6932 - acc: 0.5010\n",
      "93/93 [==============================] - 159s 2s/step - loss: 0.6932 - acc: 0.4990 - val_loss: 0.6932 - val_acc: 0.5010\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 24s 1s/step - loss: 0.6932 - acc: 0.5010\n",
      "93/93 [==============================] - 159s 2s/step - loss: 0.6932 - acc: 0.4967 - val_loss: 0.6932 - val_acc: 0.5010\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 25s 1s/step - loss: 0.6931 - acc: 0.4990\n",
      "93/93 [==============================] - 160s 2s/step - loss: 0.6933 - acc: 0.4977 - val_loss: 0.6931 - val_acc: 0.4990\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 24s 989ms/step - loss: 0.6931 - acc: 0.4990\n",
      "93/93 [==============================] - 158s 2s/step - loss: 0.6932 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.4990\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 23s 971ms/step - loss: 0.6932 - acc: 0.5010\n",
      "93/93 [==============================] - 159s 2s/step - loss: 0.6932 - acc: 0.4985 - val_loss: 0.6932 - val_acc: 0.5010\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 23s 941ms/step - loss: 0.6931 - acc: 0.4990\n",
      "93/93 [==============================] - 156s 2s/step - loss: 0.6931 - acc: 0.4999 - val_loss: 0.6931 - val_acc: 0.4990\n"
     ]
    }
   ],
   "source": [
    "sixth_history = model_6.fit_generator(train_generator_augmented,\n",
    "                              steps_per_epoch=len(train_generator_augmented), \n",
    "                              validation_data=validate_generator, \n",
    "                              validation_steps=len(validate_generator), \n",
    "                              epochs=NUM_EPOCH, verbose=VERBOSITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***References:***\n",
    "  - https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A\n",
    "  - https://github.com/Terrance-Whitehurst/Keras-Histopathologic-Cancer-Detection/blob/master/cancer-detect-keras.ipynb\n",
    "  - https://towardsdatascience.com/image-classifier-cats-vs-dogs-with-convolutional-neural-networks-cnns-and-google-colabs-4e9af21ae7a8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
